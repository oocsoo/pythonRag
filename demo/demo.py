
from to_chunk import split_into_chunks
from embedding import embed_chunk
from emb_save_db import save_embeddings
from retrieve import retrieve
from rerank import rerank


"""çŸ¥è¯†åº“å¤„ç†å­˜å…¥å‘é‡æ•°æ®åº“ä¸­ï¼"""
# å°†æ–‡ä»¶åˆ‡æˆå—ï¼è¿”å›åˆ—è¡¨
# chunks = split_into_chunks('readme.md')
# print(chunks)
# # æ¯ä¸ªå—è·å–å¯¹åº”çš„å‘é‡ï¼è¿”å›åˆ—è¡¨
# embeddings = [embed_chunk(chunk) for chunk in chunks]
#
# # å°†å—å’Œå…¶å¯¹åº”çš„å‘é‡å­˜å‚¨åˆ°æ•°æ®åº“ä¸­!
# save_embeddings(
#     chunks,
#     embeddings,
# )


"""ç”¨æˆ·æå–ä»å‘é‡æ•°æ®åº“ä¸­å¬å›ï¼"""

query = "åŒæ£€ç´¢ç­–ç•¥"
retrieved_chunks = retrieve(query, 3)
result = rerank(query, retrieved_chunks)

for i, chunk in enumerate(retrieved_chunks):
    print(f"[{i}] {chunk} \n")
print(result)

"""
#########è¾“å‡ºç»“æœ########

[0] ### ğŸ’¡ æ ¸å¿ƒä»·å€¼
- **æ™ºèƒ½è®°å¿†ç®¡ç†**ï¼šå°†åŸå§‹æ•°æ®è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„è®°å¿†çŸ¥è¯†
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šç§æ ¼å¼
- **åŒæ£€ç´¢ç­–ç•¥**ï¼šç»“åˆ RAG çš„é«˜æ•ˆæ€§å’Œ LLM çš„æ·±åº¦ç†è§£
- **è‡ªæ¼”åŒ–èƒ½åŠ›**ï¼šè®°å¿†ç»“æ„æ ¹æ®ä½¿ç”¨æ¨¡å¼è‡ªé€‚åº”ä¼˜åŒ–
 

[1] ## ğŸ“– é¡¹ç›®æ¦‚è¿° 

[2] MemU æ˜¯ä¸€ä¸ªé¢å‘ LLM å’Œ AI æ™ºèƒ½ä½“çš„è®°å¿†æ¡†æ¶ã€‚å®ƒæ¥æ”¶**å¤šæ¨¡æ€è¾“å…¥**ï¼ˆå¯¹è¯ã€æ–‡æ¡£ã€å›¾åƒï¼‰ï¼Œå°†å…¶æå–ä¸ºç»“æ„åŒ–è®°å¿†ï¼Œå¹¶ç»„ç»‡æˆ**åˆ†å±‚æ–‡ä»¶ç³»ç»Ÿ**ï¼Œæ”¯æŒ**åŸºäºåµŒå…¥çš„æ£€ç´¢ï¼ˆRAGï¼‰** å’Œ**éåµŒå…¥æ£€ç´¢ï¼ˆLLMï¼‰**ã€‚ 

{'id': '019c08453708704288a9b1a558b749d2', 'results': [{'index': 0, 'relevance_score': 0.6596528887748718}, {'index': 2, 'relevance_score': 0.0021518366411328316}, {'index': 1, 'relevance_score': 0.00012193472502985969}], 'meta': {'billed_units': {'input_tokens': 192, 'output_tokens': 0, 'search_units': 0, 'classifications': 0}, 'tokens': {'input_tokens': 192, 'output_tokens': 0}}}

"""